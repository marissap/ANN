{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to predict the output value, y, based on inputs, X. In the dataset used as an example, X is fuel prices and unemployment. Y is weekly sales. You can sub in your own data. This is supervised regression.\n",
    "\n",
    "Our inputs and outputs have different units so we have to scale data for standardization.\n",
    "\n",
    "This basic Artifical Neural Network has a hidden layer of 3 neurons. We will be using a Sigmuid Activation Function. We'll be performing Batch Gradient Descent and training on a BFGS Algorithm.\n",
    "\n",
    "Calculus used was learned in my second-year university *Multivariable Calculus* course and matrice work from first-year university *Intro to Linear Algebra* course.\n",
    "\n",
    "*Model based on the [Neural Networks Demystified](https://www.youtube.com/playlist?list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU) series by Stephen Welch.*\n",
    "\n",
    "*Dataset found [here](https://www.kaggle.com/manjeetsingh/retaildataset).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import optimize\n",
    "from matplotlib.pyplot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('./input/Features data set.csv')\n",
    "sales = pd.read_csv('./input/sales data-set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>MarkDown1</th>\n",
       "      <th>MarkDown2</th>\n",
       "      <th>MarkDown3</th>\n",
       "      <th>MarkDown4</th>\n",
       "      <th>MarkDown5</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-05-02</td>\n",
       "      <td>24924.50</td>\n",
       "      <td>False</td>\n",
       "      <td>42.31</td>\n",
       "      <td>2.572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.096358</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-12-02</td>\n",
       "      <td>46039.49</td>\n",
       "      <td>True</td>\n",
       "      <td>38.51</td>\n",
       "      <td>2.548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.242170</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>41595.55</td>\n",
       "      <td>False</td>\n",
       "      <td>39.93</td>\n",
       "      <td>2.514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.289143</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>19403.54</td>\n",
       "      <td>False</td>\n",
       "      <td>46.63</td>\n",
       "      <td>2.561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.319643</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-05-03</td>\n",
       "      <td>21827.90</td>\n",
       "      <td>False</td>\n",
       "      <td>46.50</td>\n",
       "      <td>2.625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.350143</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  Dept       Date  Weekly_Sales  IsHoliday  Temperature  Fuel_Price  \\\n",
       "0      1     1 2010-05-02      24924.50      False        42.31       2.572   \n",
       "1      1     1 2010-12-02      46039.49       True        38.51       2.548   \n",
       "2      1     1 2010-02-19      41595.55      False        39.93       2.514   \n",
       "3      1     1 2010-02-26      19403.54      False        46.63       2.561   \n",
       "4      1     1 2010-05-03      21827.90      False        46.50       2.625   \n",
       "\n",
       "   MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  \\\n",
       "0        NaN        NaN        NaN        NaN        NaN  211.096358   \n",
       "1        NaN        NaN        NaN        NaN        NaN  211.242170   \n",
       "2        NaN        NaN        NaN        NaN        NaN  211.289143   \n",
       "3        NaN        NaN        NaN        NaN        NaN  211.319643   \n",
       "4        NaN        NaN        NaN        NaN        NaN  211.350143   \n",
       "\n",
       "   Unemployment  \n",
       "0         8.106  \n",
       "1         8.106  \n",
       "2         8.106  \n",
       "3         8.106  \n",
       "4         8.106  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['Date'] = pd.to_datetime(features['Date'])\n",
    "sales['Date'] = pd.to_datetime(sales['Date'])\n",
    "\n",
    "df = pd.merge(sales,features, on=['Store','Date', 'IsHoliday'], how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.columns[[0, 1, 2, 4, 5, 7, 8, 9, 10, 11, 12]], axis=1)\n",
    "X = df[['Fuel_Price', 'Unemployment']].values\n",
    "y = df[['Weekly_Sales']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/np.amax(X, axis=0)\n",
    "y = y/np.amax(y, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Artifical Neural Network\n",
    "**hyperparameters**: constants that define structure and behaviour and are not updated as the network is trained\n",
    "\n",
    "**forward function**: propogate inputs through network using matrix. inputs are multipled with the weight on each synapses and then added together. from our dataframe above our first row in X is `[ 2.572, 8.106 ]` so the first value in the first row of our hidden layer matrix, z, will be `[ 2.572(W1) + 8.106(W1) . .  ]`. This function gives us the forumla for our hidden layer: `z2 = XW1`\n",
    "\n",
    "**weights**: representation of the strength of the connection (synpase). they determine the influence an input has on an output or level of activation. we are going to initialize with random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self):\n",
    "        # hyperparameters\n",
    "        self.inputLayerSize = 2 # fuel prices and unemployment\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayersSize = 3 # number of neurons\n",
    "        \n",
    "        # weights\n",
    "        self.W1 = np.random.randn(X.shape[1], \\\n",
    "                                 self.hiddenLayersSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayersSize, \\\n",
    "                                 self.outputLayerSize)\n",
    "            \n",
    "    def forward(self, X):\n",
    "        # propogate inputs through network\n",
    "        self.z2 = np.dot(X, self.W1) # input times weights from first layer\n",
    "        self.a2 = sigmoid(self.z2) # apply activation function\n",
    "        self.z3 = np.dot(self.a2, self.W2) # second layer of synpases\n",
    "        yhat = sigmoid(self.z3) # apply activation function\n",
    "        return yhat # output\n",
    "    \n",
    "    # helper methods we will need later on\n",
    "    def getParams(self):\n",
    "        # this is formatting\n",
    "        # returns an array of our weights\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        # this changes our weights\n",
    "        initialW1 = 0\n",
    "        finalW1 = self.hiddenLayersSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[initialW1:finalW1], (self.inputLayerSize , self.hiddenLayersSize))\n",
    "        finalW2 = finalW1 + self.hiddenLayersSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[finalW1:finalW2], (self.hiddenLayersSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = costFunctionPrime(model, X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid Activation Function\n",
    "Activation function checks the y value produced by a neuron and decides if it should be considered activated or not. A basic Step function sets a threshold where when the y value is above it, it's activated and below it's not. There is an issue though, it sets up a binary which means learning can jump around. We want something that is analog which will allow for intermediate activation that will smooth out the learning. Besides being analog the activation function we choose also has to be non-linear. This is because no matter how many layers we have applying an activation to each will just create a linear combination of linear functions meaning it will always be a function fo a single layer.\n",
    "\n",
    "The Sigmoid function introduces non-linearity and looks like a smooth step function solving both our problems. To learn more about the Sigmoid function check out [this](https://www.computing.dcu.ie/~humphrys/Notes/Neural/sigmoid.html).\n",
    "\n",
    "*This isn't to say Sigmoid funcitons don't have problems either. See [vanishing gradient](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) for more.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # apply sigmoid activation function\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testInput = np.arange(-6,6,0.01)\n",
    "plot(testInput, sigmoid(testInput), linewidth=2)\n",
    "grid(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives the next important formula: `a2 = f(z2)`\n",
    "    \n",
    "This means the activity of our second layer is equal to applying the activation function to our hidden layers. a<sup>2</sup> has the same size matrix as z<sup>2</sup>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Propogation\n",
    "We apply the activation function to our z<sup>2</sup> which is our inputs, X, times the first set of weights, W<sup>1</sup>, which are on the synapses connecting our inputs to first layer of neurons. This get's us a<sup>2</sup> as described above.\n",
    "\n",
    "To finish forward propogation we need to propgate a<sup>2</sup> all the way to the output y hat. For this we need to multiply a<sup>2</sup> by the weights of the second layer of our hidden layers, W<sup>2</sup>.\n",
    "\n",
    "We are left with `z3 = a2w2` a matrix of n rows (# of neurons) and 1 column. Finally, like the first layer we have to apply the activation function to z<sup>3</sup>. Leaving us with a formula for y hat: `ŷ = f(z3)` the official estimation of weekly sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31688864],\n",
       "       [0.31692288],\n",
       "       [0.31696885],\n",
       "       ...,\n",
       "       [0.31620342],\n",
       "       [0.31635086],\n",
       "       [0.31675405]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "yHat = model.forward(X)\n",
    "yHat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03596093],\n",
       "       [0.06642553],\n",
       "       [0.06001383],\n",
       "       ...,\n",
       "       [0.00153083],\n",
       "       [0.00109654],\n",
       "       [0.0015536 ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it is evident that are estimates are way off. That's because the model has not been trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "First step is to quantify how far off our data is. We have to determine the error with a cost function: `J = ∑ 1/2 (y - ŷ)^2` Now the goal of training is to **minimize** our cost function (minimize error). We do this by changing the weights.\n",
    "\n",
    "Here is where we break out the university-level calculus. Let's expand J so it is easier to work with, we're just going to sub in \n",
    "our input and weights. That gives us the following for our cost function `J = ∑ 1/2 (y - f(f(XW1)W2))^2`\n",
    "\n",
    "We want to figure out where the cost is going downhill (so we can find the minimum). This means we need the rate of change (slope). Derivatives give the rate of change of a function. We are changing weights (W1, W2) wrt to J so the derivative needed is dJ/dW. We are going to take our weights one at a time so we'll break that into two partial derivatives: `∂J/∂W1` and `∂J/∂W2`\n",
    "\n",
    "Gradient Descent is a process of iteratively taking steps down towards the minimum.\n",
    "\n",
    "*Note: cost function is squared because the convex nature of quadratic equations means we won't get stuck in a local minima of a non-convex equation. (Think the shape of a parabola).* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(N, X, y):\n",
    "    # compute error\n",
    "    N.yHat = N.forward(X)\n",
    "    J = 0.5*sum((y-N.yHat)**2)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation\n",
    "Backpropagation can be thought of as performing the chain rule endlessly.\n",
    "\n",
    "We will start it with our second set of weights.\n",
    "\n",
    "`∂J/∂W2 = ∂(∑ 1/2(y - ŷ)^2) / ∂W2`\n",
    "\n",
    "To simply, remove summation, we will add individual computations together later.\n",
    "\n",
    "`= 1/2(y - ŷ)^2`\n",
    "\n",
    "Apply power rule.\n",
    "\n",
    "`= (y - ŷ)`\n",
    "\n",
    "Apply chain rule. y doesn't change in the model so treat it as a constant. Move the negative from the chain rule ot the front.\n",
    "\n",
    "`= -(y - ŷ)(∂ŷ/∂W2)`\n",
    "\n",
    "Remember, ŷ is equal to the activation function applied on z3. This means we must apply the chain rule again.\n",
    "\n",
    "`= -(y - ŷ)(∂ŷ/∂W2)(∂z3/∂W2)`\n",
    "\n",
    "Differentiate Sigmoid activation function.\n",
    "\n",
    "```f(z) = 1/(1+e^(-z))\n",
    "f′(z) = (0(v) - (1(e^(-z)))) / (1(e^(-z))^2\n",
    "      = e^(-z) / (1(e^(-z))^2```\n",
    "      \n",
    "Sub differentiation result into main partial derivative.\n",
    "\n",
    "`= -(y - ŷ)(f′(z3))(∂z3/∂W2)`\n",
    "\n",
    "Before going on to the last term lets apply matrix multiplication to our first two because they are of the same time. This results in the backpropagating error, δ3 which is a matrix of [X.shape, 3]. Delta is also known as sensitivity.\n",
    "\n",
    "`δ3 = -(y - ŷ)(f′(z3))`\n",
    "\n",
    "We have are final partial derivative to compute. Remember z3 represents the activity of our second layer multiplied with the weights. Therefore, this term is equal to the activity of each synapse and can be thought of as matrix [X.shape, 3] of a2.\n",
    "\n",
    "If we transpose the matrix of the final term we are able to finsih the computing ∂J/∂W2. This is also going to take care of the summatation. *(Remember with matrix multiplication the # columns of the first matrix must equal the number of rows in the second matrix.)* Each row in the final matrix looks like `[a2(δ3) + a2(δ3) + a2(δ3) . . . ]` for the number of values in X.\n",
    "\n",
    "`∂J/∂W2 = δ3(a2T)`\n",
    "\n",
    "To summarize, we have calculated the gradient (movement down) of each term, we sum them together, and move down in the direction of the sum. Summation is what makes this process Batch Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidPrime(z):\n",
    "    # derivative of sigmoid function\n",
    "    return np.exp(-z)/((1+np.exp(-z))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunctionPrime(N, X, y):\n",
    "    # compute derivative wrt W1 and W2\n",
    "    N.yHat = N.forward(X)\n",
    "    \n",
    "    delta3 = np.multiply(-(y-N.yHat), sigmoidPrime(N.z3))\n",
    "    dJdW2 = np.dot(N.a2.T, delta3)\n",
    "    \n",
    "    # this part is explained in the text below\n",
    "    delta2 = np.dot(delta3, N.W2.T)*sigmoidPrime(N.z2)\n",
    "    dJdW1 = np.dot(X.T, delta2)\n",
    "    \n",
    "    return dJdW1, dJdW2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final computation is for our first set of weights: `∂J/∂W1` This is the movement from our inputs across synapses to neurons.\n",
    "\n",
    "`∂J/∂W1 = ∂(∑ 1/2(y - ŷ)^2) / ∂W1`\n",
    "\n",
    "Follow the derivation of W2 through until δ3 has been calculated. At this point we have the following:\n",
    "\n",
    "`= δ3(∂z3/∂W1)`\n",
    "\n",
    "For W2 the computation was finding the derivative wrt weights **on** synpases. Now, for W1, it's about taking the derivative **across** synapses. Chain rule again.\n",
    "\n",
    "`= δ3(∂z3/∂W1)(∂a2/∂W1)`\n",
    "\n",
    "We are still thinking about the relationship of z3 which represents the activity of our second layer multiplied with the weights. This time, we need to transpose the matrix of W2 weights.\n",
    "\n",
    "`= δ3(W2T)(∂a2/∂W1)`\n",
    "\n",
    "Chain rule again. This is like our earlier Sigmoid derivative except for z2 this time.\n",
    "\n",
    "`= δ3(W2T)(∂a2/∂W1)(∂z/∂W1)`\n",
    "\n",
    "`= δ3(W2T)(f′(z2))(∂z/∂W1)`\n",
    "\n",
    "The final partial derivative is our inputs. Transpose.\n",
    "\n",
    "`= δ3(W2T)(f′(z2))(XT)`\n",
    "\n",
    "Applying the derivative and adding our dJdW1s together across all our examples.\n",
    "\n",
    "`= δ2(XT)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1206.6021178 , -4107.08637481, -3568.39780452],\n",
       "       [ -923.08788321, -3088.91013053, -2631.09286304]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change our weights to decrease cost\n",
    "dJdW1, dJdW2 = costFunctionPrime(model, X, y)\n",
    "dJdW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Gradient Checking\n",
    "\n",
    "Our model gives not indication that our calculations are incorrect. The network may learn something that looks *fairly* resonsable while actually being a buggy implementation. This can lead to slowly degrading performance and inaccurate results/conclusions/analysis.\n",
    "\n",
    "*I loved this [explanation](http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/) by Standford.*\n",
    "\n",
    "So, before training we need to check our derivative procedure. We do this by thinking back to the first week of intro to calculus class where we learned the definition of a derivative (remember taking x to the limit?).\n",
    "\n",
    "`f'= limΔx→0 f(x+Δx)−f(x)/Δx`\n",
    "\n",
    "We are going to choose a middle point of our function and call the movements to the left and right as `x - ε` or `x + ε`. Essentially we are going to set Δx to a very very (not extremely) small constant called Epsilon. The calculated approximation will have an error in the range of epsilon squared.\n",
    "\n",
    "We will calculate `(f(x+epsilon)-f(x-epsilon))/(2*epsilon)` and compare to our test point.\n",
    "\n",
    "Quick terminology explainer: perturbing means adding a small random value to the weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeNumericalGradient(N, X, y):\n",
    "    paramsInitial = N.getParams()\n",
    "    ngv = np.zeros(paramsInitial.shape)    \n",
    "    perturb = np.zeros(paramsInitial.shape)\n",
    "    e = 1e-4 # very very small epsilon value\n",
    "    \n",
    "    # add epsilon to current value\n",
    "    for i in range(len(paramsInitial)):\n",
    "        perturb[i] = e\n",
    "        # move right\n",
    "        \n",
    "        N.setParams(paramsInitial + perturb) # add to current value\n",
    "        lossRight = costFunction(N, X, y) # compute cost function based on new weight\n",
    "        # move left\n",
    "        N.setParams(paramsInitial - perturb) # subtract from current value\n",
    "        lossLeft = costFunction(N, X, y) # compute cost function based on new weight\n",
    "        \n",
    "        # sub into definition\n",
    "        ngv[i] = (lossRight - lossLeft) / (2*e)\n",
    "        \n",
    "        # reset\n",
    "        perturb[i] = 0\n",
    "    \n",
    "    # reset params\n",
    "    N.setParams(paramsInitial)\n",
    "    \n",
    "    return ngv # numerical gradient vector with same num of values and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = computeNumericalGradient(model, X,  y)\n",
    "gradient = model.computeGradients(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1206.6021208 , -4107.08637266, -3568.39780048,  -923.08788795,\n",
       "       -3088.9101285 , -2631.09286176,  3968.23472927, 13315.8750929 ,\n",
       "        9600.94567796])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1206.6021178 , -4107.08637481, -3568.39780452,  -923.08788321,\n",
       "       -3088.91013053, -2631.09286304,  3968.23473019, 13315.87508975,\n",
       "        9600.94567708])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "There are many potential issues in using Gradient Descent. For example, hitting a flat spot or bouncing out of the minima because the steps are too large. The Broyden–Fletcher–Goldfarb–Shanno Numerical Optimization Algorithm (BFGS) is a popular specialization that takes the second derivative of the cost function which allows it to reduce the issues seen in traditional Gradient Descent.\n",
    "\n",
    "I recommend this [resource](https://people.duke.edu/~ccc14/sta-663/MultivariateOptimizationAlgortihms.html) from Duke University to expand your understanding of this algorithm.\n",
    "\n",
    "We are going to use a built-in scipy package which you can reference [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html).\n",
    "\n",
    "For BFGS we used a Jacobian parameter. This is simply a matrix that contains first-order partial derivatives for a vector function.\n",
    "\n",
    "In order to combat overfitting and mitigate the noise in our data, we should break up our dataset into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(X, y)\n",
    "\n",
    "trainX = trainX/np.amax(trainX, axis=0)\n",
    "trainY = trainY/np.amax(trainY, axis=0)\n",
    "testX = testX/np.amax(testX, axis=0)\n",
    "testY = testY/np.amax(testY, axis=0)\n",
    "\n",
    "trainX = np.around(trainX, 1)\n",
    "trainY = np.around(trainY, 1)\n",
    "testX = np.around(testX, 1)\n",
    "testY = np.around(testY, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model):\n",
    "        self.N = model\n",
    "    \n",
    "    def callback(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(costFunction(self.N, self.X, self.y))\n",
    "        self.testJ.append(costFunction(self.N, self.testX, self.textY))\n",
    "    \n",
    "    # optimize.minimize takes a tuple\n",
    "    def wrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = costFunction(self.N, X, y)\n",
    "        grad = self.N.computeGradients(X, y)\n",
    "        return cost, grad\n",
    "    \n",
    "    def train(self, trainX, trainY, testX, testY):\n",
    "        self.X = trainX\n",
    "        self.y = trainY\n",
    "        \n",
    "        self.testX = testX\n",
    "        self.textY = testY\n",
    "        \n",
    "        # list to store costs\n",
    "        self.J = []\n",
    "        self.testJ = []\n",
    "        \n",
    "        # initial params\n",
    "        int_params = self.N.getParams()\n",
    "        \n",
    "        # do max 200 iterations and print the convergence message\n",
    "        options = {'maxiter': 2000, 'disp': True}\n",
    "        # after each iteration we add result to our list and replace random params with trained ones (callback)\n",
    "        res = optimize.minimize(self.wrapper, int_params, jac = True, method = 'BFGS', args = (trainX, trainY), callback = self.callback, options=options)\n",
    "        \n",
    "        self.N.setParams(res.x)\n",
    "        self.optimizationResults = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 245.893805\n",
      "         Iterations: 13\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 14\n"
     ]
    }
   ],
   "source": [
    "TR = Trainer(model)\n",
    "TR.train(trainX, trainY, testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Cost')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(TR.J)\n",
    "plot(TR.testJ)\n",
    "grid(1)\n",
    "xlabel('Iterations')\n",
    "ylabel('Cost')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
